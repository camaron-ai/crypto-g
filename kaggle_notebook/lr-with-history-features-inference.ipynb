{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e0ef4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BRANCH_NAME: start_over\n",
    "# COMMIT: 1cb067b4a788e493f6eb146bf23c8bf762ac90b5\n",
    "# COMMIT_MSG: updating notebooks\n",
    "\n",
    "# MESSAGE: REMOVING MOV STATS CAUSED TOOK MUCH TIME, SIMPLER MINUTE FEAUTRES, USING ILOC INSTEAD OF LOC IN SUBMISSION\n",
    "# LASTEST_COMMIT_DATE: 2021-11-28 19:33:42\n",
    "# DATE: 2021-11-28 19:42:17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7942f4a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-28T23:31:04.652094Z",
     "start_time": "2021-11-28T23:31:04.632527Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f255bb4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-28T23:31:05.405940Z",
     "start_time": "2021-11-28T23:31:04.654759Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "136f5247",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-28T23:31:05.424068Z",
     "start_time": "2021-11-28T23:31:05.407708Z"
    }
   },
   "outputs": [],
   "source": [
    "def on_kaggle() -> bool:\n",
    "    try:\n",
    "        import gresearch_crypto\n",
    "        return True\n",
    "    except ModuleNotFoundError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "432ba6ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-28T23:31:05.444463Z",
     "start_time": "2021-11-28T23:31:05.427826Z"
    }
   },
   "outputs": [],
   "source": [
    "# HYPER PARAMETERS\n",
    "ON_KAGGLE = on_kaggle()\n",
    "SAMPLE_LEVEL = 1\n",
    "USE_SAMPLE = SAMPLE_LEVEL == 1\n",
    "USE_TOY_SAMPLE = SAMPLE_LEVEL == 2\n",
    "\n",
    "FORCE_REWRITE = True#(ON_KAGGLE and SAMPLE_LEVEL == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ee2a957",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-28T23:31:05.462514Z",
     "start_time": "2021-11-28T23:31:05.447267Z"
    }
   },
   "outputs": [],
   "source": [
    "if not ON_KAGGLE and os.path.abspath('.').endswith('notebook'):\n",
    "    os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b3c10b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-28T23:31:05.530452Z",
     "start_time": "2021-11-28T23:31:05.464996Z"
    }
   },
   "outputs": [],
   "source": [
    "# IMPORTED FROM src/preprocessing/ingest_data.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# INGEST DATA\n",
    "# DATASET DTYPES FOR SAVING MEMORY\n",
    "DTYPES = {'Asset_ID': 'int32',\n",
    "          'Open': 'float32',\n",
    "          'High': 'float32',\n",
    "          'Low': 'float32',\n",
    "          'Close': 'float32',\n",
    "          'VWAP': 'float32'}\n",
    "\n",
    "\n",
    "def merge_asset_details(df: pd.DataFrame, asset_details_path: str) -> pd.DataFrame:\n",
    "    asset_details = pd.read_csv(asset_details_path)\n",
    "    df = df.merge(asset_details[['Asset_ID', 'Asset_Name']], on='Asset_ID', how='left')\n",
    "    assert df['Asset_Name'].isna().sum() == 0, 'unexpected Asset ID'\n",
    "    return df\n",
    "\n",
    "\n",
    "def infer_dtypes(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # replace inf with NaNs\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    return df.astype(DTYPES)\n",
    "\n",
    "\n",
    "def date_to_timestamp(dates: pd.Series) -> pd.Series:\n",
    "    return dates.astype(np.int64) // 10 ** 9\n",
    "\n",
    "\n",
    "def create_valid_timestamp_range(data: pd.DataFrame, dt_col: str = 'timestamp') -> np.ndarray:\n",
    "    start_ttp, end_ttp = data[dt_col].agg(('min', 'max'))\n",
    "    return np.arange(start_ttp, end_ttp+60, 60)\n",
    "    \n",
    "\n",
    "def fill_gaps_with_timestmap(data: pd.DataFrame, dt_col: str = 'timestamp') -> pd.DataFrame:\n",
    "    assert data[dt_col].duplicated().sum() == 0, f'{dt_col} contains duplicates, cant reindex from duplicated values'\n",
    "    valid_ttp_range = create_valid_timestamp_range(data, dt_col)\n",
    "    data = data.set_index(dt_col)\n",
    "    filled_data = data.reindex(valid_ttp_range)\n",
    "    return filled_data.reset_index().rename(columns={'index': dt_col})\n",
    "\n",
    "\n",
    "def fill_gaps_crypto_data(data: pd.DataFrame,\n",
    "                          dt_col: str = 'timestamp'):\n",
    "    \n",
    "    asset_id = np.unique(data['Asset_ID'])\n",
    "    assert len(asset_id) == 1, 'expected one Asset_ID'\n",
    "    data = fill_gaps_with_timestmap(data, dt_col)\n",
    "    data['Asset_ID'] = int(asset_id[0])\n",
    "    return data\n",
    "\n",
    "\n",
    "# IMPORTED FROM src/preprocessing/feature_gen.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Callable, Dict\n",
    "from scipy.stats import linregress\n",
    "\n",
    "\n",
    "feature_dict_dtype = Dict[str, List[Callable]]\n",
    "RAW_FEATURES = ['Count', 'Open', 'High', 'Low', 'Close',\n",
    "                'Volume', 'VWAP']\n",
    "SUFFIX_FOMRAT = '__{n}min_ft'\n",
    "\n",
    "\n",
    "# FEATURE GEN FUNCTION\n",
    "def log_return(x: pd.Series, periods: int = 1) -> pd.Series:\n",
    "    return np.log(x).diff(periods=periods).fillna(0)\n",
    "\n",
    "\n",
    "def realized_volatility(series: pd.Series) -> float:\n",
    "    return np.sqrt(np.sum(np.power(series.to_numpy(), 2)))\n",
    "\n",
    "\n",
    "def linear_slope(series: pd.Series) -> float:\n",
    "    linreg = linregress(np.arange(len(series)), series)\n",
    "    return linreg.slope\n",
    "\n",
    "# UTIL\n",
    "def join_columns(columns):\n",
    "    return list(map(lambda f: '__'.join(map(str, f)), columns))\n",
    "\n",
    "BASE_FEATURES_TO_DROP = ['Open']\n",
    "def compute_instant_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    assert np.isin(RAW_FEATURES, df.columns).all(), \\\n",
    "           'missing raw features'\n",
    "\n",
    "    # normalize High and Low features\n",
    "    df['High'] = df['High'] / df['Open']\n",
    "    df['Low'] = df['Low'] / df['Open']\n",
    "    # create price features \n",
    "    # TODO: is it better to take the log or not?\n",
    "    # TODO: is it better to take the ratio or the difference?\n",
    "    # ratio will normalize features but if we use a single model for each asset will make no difference\n",
    "    # for the time, lets take the ratio\n",
    "    df['high_low_return'] = np.log1p(df['High'] / df['Low'])\n",
    "    df['open_close_return'] = np.log1p(df['Close'] / df['Open'])\n",
    "    df['upper_shadow'] = df['High'] / np.maximum(df['Close'], df['Open'])\n",
    "    df['lower_shadow'] = np.minimum(df['Close'], df['Open']) / df['Low']\n",
    "\n",
    "    # vol and count features\n",
    "    # TODO: is it useful dolar_amount?\n",
    "    df['dolar_amount'] = df['Close'] * df['Volume']\n",
    "    df['vol_per_trades'] = df['Volume'] / df['Count']\n",
    "    return df.drop(BASE_FEATURES_TO_DROP, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# # FEATURES TO COMPUTE\n",
    "# FEATURE_DICT = {'High': [np.max],\n",
    "#                 'Low': [np.min],\n",
    "#                 'Close': [np.mean],\n",
    "#                 'price_return_1': [np.sum, realized_volatility],\n",
    "#                 'vwap_return_1': [np.sum, realized_volatility],\n",
    "#                 'Count': [np.sum, np.max],\n",
    "#                 'Volume': [np.sum, np.max],\n",
    "#                 'high_low_return': [np.mean],\n",
    "#                 'open_close_return': [np.mean],\n",
    "#                }\n",
    "\n",
    "# FEATURE_DICT = {\n",
    "#                 'Close': [np.mean],\n",
    "#                }\n",
    "\n",
    "\n",
    "\n",
    "# def map_function_to_dataframe(X: pd.DataFrame,\n",
    "#                  feature_dict: feature_dict_dtype) -> Dict[str, float]:\n",
    "#     features = {f'{name}__{func.__name__}': func(X[name])\n",
    "#                 for name, func_list in feature_dict.items()\n",
    "#                 for func in func_list}\n",
    "#     return features\n",
    "\n",
    "\n",
    "# def compute_features_on_inference(X: pd.DataFrame, n: int,\n",
    "#                                  feature_dict: feature_dict_dtype) -> pd.DataFrame:\n",
    "#     features = map_function_to_dataframe(X.tail(n), feature_dict)\n",
    "#     return pd.DataFrame([features]).add_suffix(SUFFIX_FOMRAT.format(n=n)).astype(np.float32)\n",
    "\n",
    "\n",
    "# def compute_features_on_train(X: pd.DataFrame, n: int,\n",
    "#                              feature_dict: feature_dict_dtype) -> pd.DataFrame:\n",
    "#     assert X['Asset_ID'].nunique() == 1, \\\n",
    "#            'expected only one Asset_ID'\n",
    "    \n",
    "#     mov_features = X.rolling(n, min_periods=1).agg(feature_dict)\n",
    "#     mov_features.columns = join_columns(mov_features.columns)\n",
    "#     mov_features = mov_features.add_suffix(SUFFIX_FOMRAT.format(n=n))\n",
    "    \n",
    "#     assert len(mov_features) == len(X), 'output lenght do not match the input lenght'\n",
    "#     return mov_features.astype(np.float32)\n",
    "\n",
    "\n",
    "# IMPORTED FROM src/preprocessing/__init__.py\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import gc\n",
    "from typing import Tuple, Dict, Any\n",
    "\n",
    "\n",
    "EXPECTED_RAW_COLS = ['timestamp', 'Asset_ID', 'Count',\n",
    "                     'Open', 'High', 'Low', 'Close',\n",
    "                     'Volume', 'VWAP']\n",
    "\n",
    "# def process_train_data(df: pd.DataFrame,\n",
    "#                        window: int = 60) -> pd.DataFrame:\n",
    "#     asset_ids = sorted(df['Asset_ID'].unique())\n",
    "    \n",
    "#     global_features = []\n",
    "#     for asset_id in asset_ids:\n",
    "#         print(f'processing asset_id={asset_id}')\n",
    "#         raw_local_data = df.query(\"Asset_ID==@asset_id\").reset_index(drop=True)\n",
    "#         # fill nan gaps\n",
    "#         raw_local_data = fill_gaps_crypto_data(raw_local_data)\n",
    "#         raw_local_data = infer_dtypes(raw_local_data)\n",
    "#         # base features\n",
    "#         raw_features = compute_base_features(raw_local_data)\n",
    "        \n",
    "#         # compute history features\n",
    "#         start_time = time.time()\n",
    "#         features = compute_features_on_train(raw_features, window, FEATURE_DICT)\n",
    "#         elapsed_time = (time.time() - start_time) / 60\n",
    "        \n",
    "#         print(f'elapsed time: {elapsed_time:.4f}min')\n",
    "#         # add timestamp\n",
    "#         features['timestamp'] = raw_features['timestamp'].to_numpy()\n",
    "#         features['Asset_ID'] = asset_id\n",
    "#         global_features.append(features)\n",
    "\n",
    "#         del raw_local_data, raw_features\n",
    "#         gc.collect()\n",
    "#     print('joining datasets')\n",
    "#     global_features = pd.concat(global_features, axis=0, ignore_index=True)\n",
    "#     assert global_features['Asset_ID'].nunique() == len(asset_ids), \\\n",
    "#            f'missing Asset_IDs'\n",
    "#     return global_features\n",
    "\n",
    "\n",
    "# def process_test_data(test_dict: Dict[str, float], local_history_df: pd.DataFrame,\n",
    "#                       window: int = 60) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "\n",
    "#     last_timestamp = local_history_df.iloc[-1]['timestamp']\n",
    "#     current_timestamp = test_dict['timestamp']\n",
    "#     # add new observation forget the last first row\n",
    "#     local_history_df = local_history_df.append([test_dict], ignore_index=True)\n",
    "#     minute_diff = (current_timestamp - last_timestamp) // 60\n",
    "\n",
    "#     assert minute_diff > 0, f'current timestamp included in history df, {current_timestamp} <= {last_timestamp}'\n",
    "\n",
    "#     if minute_diff > 1:\n",
    "#         print(f'missing more than one minut of data, missing minutes: {minute_diff}')\n",
    "#         print(f'filling gaps')\n",
    "#         local_history_df = fill_gaps_crypto_data(local_history_df)\n",
    "#     raw_features = compute_base_features(local_history_df)\n",
    "#     features = compute_features_on_inference(raw_features, n=window, feature_dict=FEATURE_DICT)\n",
    "\n",
    "#     return features, local_history_df\n",
    "\n",
    "\n",
    "def test_submission_format(submission: pd.DataFrame, expected_len: int = 14):\n",
    "    assert list(submission.columns) == ['row_id', \"Target\"], 'submission do not match expected columns'\n",
    "    assert len(submission) == expected_len, 'submission do not match expected lenght'\n",
    "    assert submission['Target'].isna().sum() == 0, 'target includes NaNs'\n",
    "    assert submission['row_id'].dtype == np.int32\n",
    "    assert submission['Target'].dtype == np.float64\n",
    "    assert submission['Target'].isna().sum() == 0, 'submission contains NaN values'\n",
    "    assert np.isinf(submission['Target']).sum() == 0 ,'submission contains inf values'\n",
    "\n",
    "\n",
    "def inference(test_data: pd.DataFrame, submission: pd.DataFrame,\n",
    "             models: Dict[str, Any],\n",
    "             ) -> pd.DataFrame:\n",
    "    expected_len = len(submission)\n",
    "    test_data = infer_dtypes(test_data)\n",
    "    features = compute_instant_features(test_data.loc[:, EXPECTED_RAW_COLS])\n",
    "    records = features.to_dict('records')\n",
    "    for index, asset_features in enumerate(records):\n",
    "        # get the asset ID\n",
    "        asset_id = int(asset_features['Asset_ID'])\n",
    "        assert asset_id in models, f'{asset_id} not in TRAINED MODELS'\n",
    "        # get model\n",
    "        model = models[asset_id]\n",
    "        asset_frame = pd.DataFrame([asset_features])\n",
    "        local_test_yhat = model.predict(asset_frame)\n",
    "        # add to submission format\n",
    "        submission.iloc[index, 1] = local_test_yhat[0]\n",
    "    # testing submission format\n",
    "    test_submission_format(submission, expected_len=expected_len)\n",
    "    return submission\n",
    "\n",
    "\n",
    "# IMPORTED FROM src/metrics.py\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "ASSET_WEIGHT = {\n",
    "'Bitcoin Cash': 2.3978952727983707,\n",
    "'Binance Coin': 4.30406509320417,\n",
    "'Bitcoin': 6.779921907472252,\n",
    "'EOS.IO': 1.3862943611198906,\n",
    "'Ethereum Classic': 2.079441541679836,\n",
    "'Ethereum': 5.8944028342648505,\n",
    "'Litecoin': 2.3978952727983707,\n",
    "'Monero': 1.6094379124341005,\n",
    "'TRON': 1.791759469228055,\n",
    "'Stellar': 2.079441541679836,\n",
    "'Cardano': 4.406719247264253,\n",
    "'IOTA': 1.0986122886681098,\n",
    "'Maker': 1.0986122886681098,\n",
    "'Dogecoin': 3.555348061489413}\n",
    "\n",
    "\n",
    "TOTAL_WEIGHT_SUM = sum(ASSET_WEIGHT.values())\n",
    "\n",
    "#### weighted correlation cofficient\n",
    "def compute_weighted_mean(x: np.ndarray, w: np.ndarray) -> float:\n",
    "    return np.average(x, weights=w)\n",
    "\n",
    "\n",
    "def compute_weighted_var(x: np.ndarray, w: np.ndarray) -> float:\n",
    "    mean = compute_weighted_mean(x, w)\n",
    "    dev = np.square(x - mean)\n",
    "    return compute_weighted_mean(dev, w)\n",
    "\n",
    "\n",
    "def compute_weighted_cov(y: np.ndarray, yhat: np.ndarray, w: np.ndarray) -> float:\n",
    "    y_mean = compute_weighted_mean(y, w)\n",
    "    yhat_mean = compute_weighted_mean(yhat, w)\n",
    "    return compute_weighted_mean((y - y_mean) * (yhat - yhat_mean), w)\n",
    "\n",
    "\n",
    "def compute_weighted_corr(y: np.ndarray, yhat: np.ndarray,\n",
    "                          w: np.ndarray = None) -> float:\n",
    "    if w is None:\n",
    "        w = np.ones(len(y))\n",
    "    assert len(y) == len(yhat)\n",
    "    var_y = compute_weighted_var(y, w)\n",
    "    var_yhat = compute_weighted_var(yhat, w)\n",
    "    \n",
    "    return compute_weighted_cov(y, yhat, w) / np.sqrt(var_y * var_yhat)\n",
    "\n",
    "\n",
    "def compute_correlation(df: pd.DataFrame,\n",
    "                        target_name: str = 'Target',\n",
    "                        yhat_name: str = 'yhat',\n",
    "                        group_col: str = 'Asset_ID') -> pd.DataFrame:\n",
    "    def _spearman_corr(d: pd.DataFrame):\n",
    "        return np.corrcoef(d[target_name], d[yhat_name])[0, 1]\n",
    "    \n",
    "    assert df[target_name].isna().sum() == 0, f'{target_name} includes NaN'\n",
    "    corrs = df.groupby(group_col).apply(_spearman_corr)\n",
    "    return corrs.to_frame('corr').reset_index()\n",
    "\n",
    "\n",
    "def compute_sharpe(df: pd.DataFrame,\n",
    "                   period: int = 60*24*7,   # weekly\n",
    "                   target_name: str = 'Target',\n",
    "                   yhat_name: str = 'yhat',\n",
    "                   weight_name: str = 'weight',\n",
    "                   ) -> float:\n",
    "    \n",
    "    timesteps = (df['timestamp'].max() - df['timestamp']) // 60   # from 0 up to n min,\n",
    "    time_groups = timesteps // period\n",
    "    corrs = df.groupby(time_groups).apply(lambda d: compute_weighted_corr(y=d[target_name].to_numpy(),\n",
    "                                                                          yhat=d[yhat_name].to_numpy(),\n",
    "                                                                          w=d[weight_name].to_numpy()))\n",
    "    assert np.isnan(corrs).sum() == 0, 'period corrs contains NaN values'\n",
    "    mean = corrs.mean()\n",
    "    std = corrs.std()\n",
    "    return {'sharpe': mean / (std + 1e-15), 'corr_period_mean': mean, 'corr_period_std': std}\n",
    "\n",
    "\n",
    "def compute_metrics(df: pd.DataFrame,\n",
    "                    target_name: str = 'Target',\n",
    "                    yhat_name: str = 'yhat',\n",
    "                    group_col: str = 'Asset_Name') -> Tuple[pd.Series, pd.DataFrame]:\n",
    "\n",
    "    # BASE APPROACH, COMPUTE CORR AND THE WEIGHTED THEM\n",
    "    corrs_df = compute_correlation(df, target_name=target_name,\n",
    "                                   yhat_name=yhat_name,\n",
    "                                   group_col=group_col)\n",
    "    corrs_df['weight'] = corrs_df[group_col].map(ASSET_WEIGHT)\n",
    "    corrs_df['weighted_corr'] = corrs_df[['corr', 'weight']].prod(axis=1)\n",
    "    corr = corrs_df['weighted_corr'].sum() / TOTAL_WEIGHT_SUM\n",
    "    \n",
    "    corr_stats = corrs_df['corr'].agg(('min', 'max', 'std')).add_prefix('corr_').to_dict()\n",
    "    # COMPUTE WEIGHTED CORRELATION USING FORMULA\n",
    "    df['_weight'] = df[group_col].map(ASSET_WEIGHT)\n",
    "    theor_corr = compute_weighted_corr(y=df[target_name], yhat=df[yhat_name], w=df['_weight'].to_numpy())\n",
    "    # DIVIDE IT INTO DAILY CHUNKS AND COMPUTE SHARPE\n",
    "    sharpe_scores = compute_sharpe(df, target_name=target_name, yhat_name=yhat_name, weight_name='_weight')\n",
    "    scores = {'theor_corr': theor_corr, 'weighted_corr': corr}\n",
    "    scores.update(sharpe_scores)\n",
    "    scores.update(corr_stats)\n",
    "    df.drop('_weight', axis=1, inplace=True)\n",
    "    return pd.Series(scores), corrs_df\n",
    "\n",
    "\n",
    "# IMPORTED FROM src/cv.py\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from typing import List, Tuple\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "def get_date_range(dates: pd.Series):\n",
    "    return dates.agg(('min', 'max'))\n",
    "\n",
    "\n",
    "class TimeSeriesSplit(_BaseKFold):\n",
    "    def __init__(self, periods: List[Tuple[str, str]],\n",
    "                 train_days: int = None,\n",
    "                 gap: int = 1,\n",
    "                 gap_unit: int = 'd',\n",
    "                 dt_col: str = 'date'):\n",
    "        self.dt_col = dt_col\n",
    "        self.periods = periods\n",
    "        self.train_days = train_days\n",
    "        self.gap = gap\n",
    "        self.gap_unit = gap_unit\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.periods)\n",
    "    \n",
    "    def check_input(self, X: pd.DataFrame, y=None, groups=None):\n",
    "        assert self.dt_col in X.columns, f'{self.dt_col} do not exits in input dataframe'\n",
    "        \n",
    "    def split(self, X: pd.DataFrame, y=None, groups=None):\n",
    "        dates = X[self.dt_col]\n",
    "        self.check_input(X)\n",
    "        \n",
    "        first_date = dates.min()\n",
    "        \n",
    "        indices = np.arange(len(X))\n",
    "        for period in self.periods:\n",
    "            first_valid_date = pd.to_datetime(period[0])\n",
    "            \n",
    "            last_train_date = first_valid_date - pd.to_timedelta(self.gap, unit=self.gap_unit)\n",
    "            \n",
    "            if self.train_days:\n",
    "                first_train_date = last_train_date - pd.to_timedelta(self.train_days, unit='d')\n",
    "                first_train_date = np.maximum(first_train_date, first_date)\n",
    "            else:\n",
    "                first_train_date = first_date\n",
    "            \n",
    "            valid_mask = dates.between(*period)\n",
    "            train_mask = (dates.between(first_train_date, last_train_date)) & (dates < first_valid_date)\n",
    "            \n",
    "            yield indices[train_mask], indices[valid_mask]\n",
    "\n",
    "\n",
    "\n",
    "def gen_eval_periods(start_date: str,\n",
    "                     n_test: int,\n",
    "                     n_splits: int,\n",
    "                     unit: str = 'd') -> List[Tuple[datetime, datetime]]:\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    eval_periods = []\n",
    "    for _ in range(n_splits):\n",
    "        end_date = start_date + pd.to_timedelta(n_test, unit=unit)\n",
    "        eval_periods.append([start_date, end_date])\n",
    "        start_date = end_date + pd.to_timedelta(1, unit=unit)\n",
    "    return eval_periods\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dcd141e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-28T23:31:05.550740Z",
     "start_time": "2021-11-28T23:31:05.532580Z"
    }
   },
   "outputs": [],
   "source": [
    "if ON_KAGGLE:\n",
    "    RAW_DIR = Path('../input/g-research-crypto-forecasting/')\n",
    "    SAMPLE_DIR = Path('../input/create-sample-dataset/data/raw/sample/')\n",
    "    TOY_SAMPLE_DIR = Path('../input/create-sample-dataset/data/raw/toy_sample/')\n",
    "else:\n",
    "    RAW_DIR =  Path('data/raw')\n",
    "    TOY_SAMPLE_DIR = RAW_DIR.joinpath('toy_sample')\n",
    "    SAMPLE_DIR = RAW_DIR.joinpath('sample')\n",
    "\n",
    "# filename\n",
    "TRAIN_FILE = 'train.csv'\n",
    "ASSET_DETAILS_PATH = RAW_DIR / 'asset_details.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74f7a9ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-28T23:31:05.571846Z",
     "start_time": "2021-11-28T23:31:05.553699Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING SAMPLE DATASET\n"
     ]
    }
   ],
   "source": [
    "if USE_TOY_SAMPLE:\n",
    "    print('USING TOY DATASET')\n",
    "    RAW_TRAIN_PATH = TOY_SAMPLE_DIR / TRAIN_FILE\n",
    "\n",
    "elif USE_SAMPLE:\n",
    "    print('USING SAMPLE DATASET')\n",
    "    RAW_TRAIN_PATH = SAMPLE_DIR / TRAIN_FILE\n",
    "\n",
    "else:\n",
    "    print('USING RAW DATASET')\n",
    "    RAW_TRAIN_PATH = RAW_DIR / TRAIN_FILE\n",
    "\n",
    "assert RAW_TRAIN_PATH.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28ef35af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-28T23:31:05.709303Z",
     "start_time": "2021-11-28T23:31:05.574680Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asset_details.csv\t\t   sample\r\n",
      "example_sample_submission.csv\t   supplemental_train.csv\r\n",
      "example_test.csv\t\t   toy_sample\r\n",
      "g-research-crypto-forecasting.zip  train.csv\r\n",
      "gresearch_crypto\r\n"
     ]
    }
   ],
   "source": [
    "!ls {RAW_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b479f0e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-28T23:31:04.738Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "raw_data = pd.read_csv(RAW_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c53d2f",
   "metadata": {},
   "source": [
    "## create train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9313bf55",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-28T23:31:04.740Z"
    }
   },
   "outputs": [],
   "source": [
    "PREPRO_PARAMS = {'window': 60}\n",
    "MAIN_INDEX = ['timestamp', 'Asset_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddebc9cc",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-28T23:31:04.741Z"
    }
   },
   "outputs": [],
   "source": [
    "# get valid data only, drop where the target is NaN \n",
    "data = raw_data[MAIN_INDEX + ['Target']].dropna(subset=['Target'])\n",
    "# format time to human readable \n",
    "data['time'] = pd.to_datetime(data['timestamp'], unit='s')\n",
    "# merge asset names\n",
    "data = merge_asset_details(data, ASSET_DETAILS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c771517",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-28T23:31:04.744Z"
    }
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37a820f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-28T23:31:04.747Z"
    }
   },
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5631261",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-28T23:31:04.750Z"
    }
   },
   "outputs": [],
   "source": [
    "features_df = raw_data.loc[:, EXPECTED_RAW_COLS]\n",
    "features_df = infer_dtypes(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c298ad8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-28T23:31:04.753Z"
    }
   },
   "outputs": [],
   "source": [
    "features_df = compute_instant_features(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35b2b02",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-28T23:31:04.755Z"
    }
   },
   "outputs": [],
   "source": [
    "features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45768728",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-28T23:31:04.757Z"
    }
   },
   "outputs": [],
   "source": [
    "assert features_df['timestamp'].isna().sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8898b5",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-28T23:31:04.759Z"
    }
   },
   "outputs": [],
   "source": [
    "data = data.merge(features_df, on=MAIN_INDEX, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7fe91c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-28T23:31:04.760Z"
    }
   },
   "outputs": [],
   "source": [
    "data.isna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44d5f1a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-28T23:31:04.763Z"
    }
   },
   "outputs": [],
   "source": [
    "list(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6b551d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-28T23:31:04.765Z"
    }
   },
   "outputs": [],
   "source": [
    "FEATURES = [\n",
    "'Count',\n",
    "'High',\n",
    "'Low',\n",
    "'Close',\n",
    "'Volume',\n",
    "'VWAP',\n",
    "'high_low_return',\n",
    "'open_close_return',\n",
    "'upper_shadow',\n",
    "'lower_shadow',\n",
    "'dolar_amount',\n",
    "'vol_per_trades'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcd4e69",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-28T23:31:04.767Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Lasso, Ridge\n",
    "from sklearn.preprocessing import MinMaxScaler, FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class FilterFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,\n",
    "                 features: List[str] = None,\n",
    "                 sort: bool = False):\n",
    "        self.sort = sort\n",
    "        self.features = features[:]\n",
    "        if self.sort:\n",
    "            self.features.sort()\n",
    "    \n",
    "    def fit(self, X: pd.DataFrame, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: pd.DataFrame):\n",
    "        return X.loc[:, self.features]\n",
    "\n",
    "    \n",
    "def test_filter_features():\n",
    "    n_feat = 100\n",
    "    size = 500\n",
    "    t = 100\n",
    "    expected_columns = [f'{f}_ft' for f in np.arange(n_feat)]\n",
    "    \n",
    "    expected_df = pd.DataFrame(np.random.randn(size, n_feat), columns=expected_columns)\n",
    "    tmf = FilterFeatures(expected_columns)\n",
    "    tmf.fit(expected_df)\n",
    "    \n",
    "    for i in range(t):\n",
    "        shuffle_columns = np.random.permutation(expected_columns)\n",
    "        shuffle_df = expected_df.loc[:, shuffle_columns]\n",
    "        actual_columns = list(tmf.transform(shuffle_df).columns)\n",
    "        assert (actual_columns == expected_columns), f'cols do not match at iter {i}'\n",
    "    \n",
    "test_filter_features()\n",
    "\n",
    "def build_model(params={'alpha': 0.001}):\n",
    "    model = Pipeline([('filter', FilterFeatures(FEATURES)),\n",
    "                      ('norm', MinMaxScaler()),\n",
    "                      ('model', Ridge(params['alpha']))])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c9a4b9",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-28T23:31:04.768Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(config, train_data, valid_data, pipeline=None):\n",
    "    model  = build_model(config['model'])\n",
    "\n",
    "    if config['training']['time_decay_alpha'] is not None:\n",
    "        time_decay_alpha = config['training']['time_decay_alpha']\n",
    "        print(f'using exponential_time_decay with alpha {time_decay_alpha}')\n",
    "        timesteps = ((train_data['timestamp'].max() - train_data['timestamp'])//60//60//24)\n",
    "        weight = time_decay_alpha ** timesteps\n",
    "    else:\n",
    "        weight = None\n",
    "\n",
    "    model.fit(train_data[FEATURES], train_data['Target'], model__sample_weight=weight)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fec683f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-28T23:31:04.770Z"
    }
   },
   "outputs": [],
   "source": [
    "PULIC_LB_RANGE = ['2021-06-13 00:00:00',\n",
    "                  '2021-09-13 00:00:00'] # 3 MONTH WORTH OF DATA\n",
    "\n",
    "if USE_TOY_SAMPLE:\n",
    "    EVAL_PERIODS = [['2021-09-15', '2021-09-22']]\n",
    "\n",
    "else:\n",
    "    EVAL_PERIODS = [PULIC_LB_RANGE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fa6271",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-28T23:31:04.772Z"
    }
   },
   "outputs": [],
   "source": [
    "CV_PARAMS = {'gap_unit': 'min', 'dt_col': 'time'}\n",
    "\n",
    "CV = TimeSeriesSplit(EVAL_PERIODS, **CV_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb6ebfd",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-28T23:31:04.773Z"
    }
   },
   "outputs": [],
   "source": [
    "train_idx, valid_idx = next(iter(CV.split(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6d6858",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-28T23:31:04.774Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = data.loc[train_idx, :].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7350c2c7",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-28T23:31:04.776Z"
    }
   },
   "outputs": [],
   "source": [
    "valid_data = data.loc[valid_idx, :].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19c1ce2",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-28T23:31:04.778Z"
    }
   },
   "outputs": [],
   "source": [
    "get_date_range(train_data['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d2808e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-28T23:31:04.779Z"
    }
   },
   "outputs": [],
   "source": [
    "get_date_range(valid_data['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657cc10f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-28T23:31:04.781Z"
    }
   },
   "outputs": [],
   "source": [
    "MODEL_CONFIG = {'alpha': 0.001}\n",
    "TRAIN_CONFIG = {'time_decay_alpha': 0.99}\n",
    "CONFIG = {'model': MODEL_CONFIG, 'training': TRAIN_CONFIG}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c649420",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-28T23:31:04.782Z"
    }
   },
   "outputs": [],
   "source": [
    "MODELS = {}\n",
    "for asset_id, train_asset_data in data.groupby(\"Asset_ID\"):\n",
    "    print(f'training model for asset_ID == {asset_id}')\n",
    "    train_asset_data = train_asset_data.reset_index(drop=True)    \n",
    "    model = train_model(CONFIG, train_asset_data, train_asset_data)\n",
    "    MODELS[asset_id] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43650874",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-28T23:31:04.784Z"
    }
   },
   "outputs": [],
   "source": [
    "if not ON_KAGGLE:\n",
    "    sys.path.append(str(RAW_DIR))\n",
    "\n",
    "import gresearch_crypto\n",
    "import traceback\n",
    "env = gresearch_crypto.make_env()   # initialize the environment\n",
    "iter_test = env.iter_test()    # an iterator which loops over the test set and sample submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a004439",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-28T23:31:04.786Z"
    }
   },
   "outputs": [],
   "source": [
    "for i, (raw_test_df, submission) in enumerate(iter_test):\n",
    "\n",
    "    submission = inference(test_data=raw_test_df, submission=submission,\n",
    "                            models=MODELS)\n",
    "    if i % 1000 == 0 or i < 10:\n",
    "        display(submission)    \n",
    "    env.predict(submission)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
